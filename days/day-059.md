Absolutely ğŸ”¥ â€” here is your **personalized README**, written based on the exact troubleshooting flow *you* followed on the cluster.

---

# ğŸš‘ Fixing Redis Deployment Failure in Kubernetes

Last week, the Nautilus DevOps team deployed a Redis application on the Kubernetes cluster. It was running correctly until a team member introduced configuration mistakes in the existing setup. As a result, the `redis-deployment` pods stopped working and were stuck in a non-running state.

The objective was to investigate the issue directly from the `jump_host` and restore the application.

> âš ï¸ Note: `kubectl` on `jump_host` is already configured to access the cluster.

---

# ğŸ” Step-by-Step Troubleshooting Process

---

## 1ï¸âƒ£ Check Pod Logs

First, we attempted to check container logs:

```bash
kubectl logs redis-deployment-54cdf4f76d-zx56k
```

Result:

```
container "redis-container" is waiting to start: ContainerCreating
```

âš ï¸ This means the container never started â€” so logs are unavailable.

When logs fail, the next step is always:

---

## 2ï¸âƒ£ Inspect Pod YAML

```bash
kubectl get pod redis-deployment-54cdf4f76d-zx56k -o yaml
```

### ğŸ” Critical Findings

Two configuration errors were identified:

---

### âŒ Issue 1 â€” Wrong Image Tag

```yaml
image: redis:alpin
```

Correct tag:

```
redis:alpine
```

The tag `alpin` does **not exist**, so Kubernetes could not pull the image.

---

### âŒ Issue 2 â€” Incorrect ConfigMap Name

```yaml
name: redis-conig
```

Correct name:

```
redis-config
```

The deployment was referencing a ConfigMap that does not exist.

This caused volume mount failure during pod initialization.

---

# ğŸ›  Fix Implementation

---

## 3ï¸âƒ£ Edit the Deployment

Instead of modifying the pod directly (since it is managed by a ReplicaSet), we edited the deployment:

```bash
kubectl edit deployment redis-deployment
```

### Fix 1 â€” Correct Image

```yaml
image: redis:alpine
```

### Fix 2 â€” Correct ConfigMap Name

```yaml
name: redis-config
```

Save and exit.

---

## 4ï¸âƒ£ Restart the Deployment

```bash
kubectl rollout restart deployment redis-deployment
```

---

## 5ï¸âƒ£ Verify Pod Status

```bash
kubectl get pods
```

Expected output:

```
STATUS: Running
```

---

## 6ï¸âƒ£ Confirm Deployment Health

```bash
kubectl get deployment redis-deployment
```

Expected:

```
READY   UP-TO-DATE   AVAILABLE
1/1     1            1
```

Redis application successfully restored âœ…

---

# ğŸ§  Root Cause Summary

| Issue                                | Impact                                      |
| ------------------------------------ | ------------------------------------------- |
| Wrong image tag (`alpin`)            | Image pull failed â†’ container never created |
| Misspelled ConfigMap (`redis-conig`) | Volume mount failed â†’ pod stuck in Pending  |

This was a classic DevOps typo incident affecting both container image resolution and volume configuration.

---

# ğŸ“š Good to Know (From This Incident)

## ğŸ” Why `kubectl logs` Didn't Work

Logs only work when a container has started.
If a pod is stuck in `ContainerCreating`, the issue is typically:

* Image pull failure
* Volume mount failure
* Missing ConfigMap or Secret
* Storage/PVC issue

---

## ğŸ§­ Understanding Pod Phases vs Container States

**Pod Phase**

* `Pending` â†’ Not fully scheduled or initializing
* `Running` â†’ At least one container running
* `Failed` â†’ Pod terminated unsuccessfully

**Container State**

* `Waiting` â†’ Not started yet
* `Running` â†’ Executing normally
* `Terminated` â†’ Finished or crashed

In our case:

```
Phase: Pending
State: Waiting (ContainerCreating)
```

This tells us the failure happened **before application runtime**.

---

## ğŸ§ª Systematic Troubleshooting Pattern (What We Applied)

When a Kubernetes app goes down:

1. `kubectl get pods`
2. `kubectl logs <pod>`
3. If logs unavailable â†’ `kubectl describe pod`
4. If still unclear â†’ inspect full YAML
5. Validate:

   * Image names
   * ConfigMap/Secret names
   * Volume mounts
   * Ports
6. Fix at the Deployment level
7. Restart rollout
8. Verify health

This structured approach prevents random guessing.

---

# ğŸš€ Lessons Learned

* Small typos can break production workloads.
* Always validate image tags before applying changes.
* ConfigMap and Secret names must match exactly.
* Never edit pods directly â€” always fix the Deployment.
* YAML precision matters in Kubernetes.

---

# ğŸ Final Result

âœ” Redis image corrected
âœ” ConfigMap reference fixed
âœ” Deployment restarted
âœ” Pods back to `Running` state

Cluster restored successfully ğŸ”¥

---

If you'd like, I can also generate a version formatted for GitHub with badges and cleaner markdown styling.
